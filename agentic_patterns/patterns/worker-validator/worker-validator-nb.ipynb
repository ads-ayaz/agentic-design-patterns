{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a24922",
   "metadata": {},
   "source": [
    "# WORKERâ€“VALIDATOR | Agentic Pattern\n",
    "\n",
    "A two-agent pattern that introduces a quality control loop. A Worker agent attempts the task, and a Validator agent reviews the result against the original instructions. If the result is inadequate, the Validator provides feedback, and the Worker revises the output. This loop continues until the Validator approves the result or a termination condition is reached.\n",
    "\n",
    "## Problem Addressed\n",
    "This pattern improves the reliability and quality of output for tasks that may require iterative refinement, quality assurance, or structured reasoning. It is especially useful when the cost of error is high or task success criteria are non-trivial to evaluate.\n",
    "\n",
    "## Pattern Structure\n",
    "\n",
    "**Agents |**\n",
    "- `Worker`: Interprets the task and produces an initial output.\n",
    "- `Validator`: Evaluates whether the Workerâ€™s output satisfies the task requirements, and provides structured feedback if not.\n",
    "\n",
    "**Coordination Topology |**\n",
    "- Sequential feedback loop.\n",
    "- `Caller â†’ Worker â†’ Validator â†’ (loop: Worker â† Validator)` until the Validator approves the result or a max iteration limit is reached.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "Worker:\n",
    "- Has all the assumptions of the Worker pattern (tools, reasoning, etc.).\n",
    "- Can **interpret and apply feedback** to revise prior outputs.\n",
    "\n",
    "Validator:\n",
    "- Possesses the capability to **interpret task requirements**, **evaluate the Workerâ€™s output**, and **generate feedback or approval**.\n",
    "- Can compare against a reference prompt or desired criteria.\n",
    "\n",
    "Agents may or may not share memory. Coordination state must persist across iterations.\n",
    "\n",
    "## Inputs\n",
    "- A task description (e.g., instructions or goals).\n",
    "- Optionally: a rubric or evaluation criteria for the Validator.\n",
    "\n",
    "## Outputs\n",
    "- A final validated result.\n",
    "- Optionally: Validatorâ€™s rationale or evaluation score.\n",
    "\n",
    "## Behavioural Flow\n",
    "\n",
    "1. Input task is passed to the Worker.\n",
    "2. Worker generates initial output.\n",
    "3. Output and task are passed to the Validator.\n",
    "4. Validator evaluates the output:\n",
    "    - If acceptable, the result is returned.\n",
    "    - If not, Validator returns structured feedback.\n",
    "5. Worker revises output based on feedback.\n",
    "6. Loop returns to step 3. Repeat until approval or max iterations reached.\n",
    "\n",
    "## Strengths\n",
    "- Higher output quality and reliability.\n",
    "- Supports more complex, ambiguous, or high-stakes tasks.\n",
    "- Validator role introduces an externalized standard of correctness or fit.\n",
    "\n",
    "## Weaknesses\n",
    "- Higher latency and compute cost.\n",
    "- Requires well-aligned Validator to avoid false negatives or noise.\n",
    "- Coordination complexity increases with number of iterations.\n",
    "\n",
    "## Variations and Extensions\n",
    "- **Multi-round Validator**: Uses temperature annealing or scoring thresholds to control the feedback loop.\n",
    "- **Rotating Roles**: Worker and Validator swap roles periodically to simulate peer review.\n",
    "- **Planner-Worker-Validator**: Validator ensures both planning and execution meet the goal.\n",
    "\n",
    "## Example Use Cases\n",
    "- Generate a product description from specifications.\n",
    "- Answer a factual question using embedded knowledge.\n",
    "- Write a function to perform a specific operation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da2301",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Create a `.env` file in the root directory of this project for private configuration variables (API keys, etc). Running this notebook requires a valid `OPENAI_API_KEY=sk-proj-XXXX`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844816fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "agent_model_DEFAULT = os.getenv('CONF_OPENAI_DEFAULT_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7b625",
   "metadata": {},
   "source": [
    "Generate the `agents-config.yaml` configuration file that specifies the agents to be created for this pattern. The file includes the instructions prompt, model and other specifications for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60306404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tmp/agents-config.yaml\n",
    "# agents-config.yaml\n",
    "worker:\n",
    "  name: Basic Worker\n",
    "  instructions: |\n",
    "    You are a single-step Worker. Execute the given task directly and return the best possible result.\n",
    "    Do not ask clarifying questions. Do not handoff, delegate, or call tools unless explicitly provided.\n",
    "    Prefer concise, correct outputs. Use Markdown formatting when helpful.\n",
    "  model: openai/gpt-4.1-nano\n",
    "  has_memory: False\n",
    "  temperature: 0.4\n",
    "  max_tokens: 1000\n",
    "\n",
    "validator:\n",
    "  name: Basic Validator\n",
    "  instructions: |\n",
    "    You are a Validator agent in a Workerâ€“Validator loop. Your role is to **assess whether \n",
    "    the Workerâ€™s output meets the task objective and success criteria**.\n",
    "    \n",
    "    Your job is **not to redo the research yourself**, but to:\n",
    "    1. Decide if the output is acceptable given the task and whatever success criteria is provided.\n",
    "    2. If not, give precise, actionable feedback so the Worker can correct it.\n",
    "  model: openai/gpt-4o-mini\n",
    "  temperature: 0.2\n",
    "  max_tokens: 1000\n",
    "  output_type: ValidatorResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317333a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structured output type for Validator\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "class ValidatorResponse(BaseModel):\n",
    "    status: Literal[\"approved\", \"needs_revision\"]  = Field(..., description='Must be either \"approved\" or \"needs_revision\"')\n",
    "    rationale: str = Field(..., description='Summarise your evaluation in 10 words, referencing specific success criteria.')\n",
    "    feedback: Optional[str] = Field(None, description='Only include if status is \"needs_revision\". Provide concrete, actionable instructions for the Worker to correct or improve the output. Be explicit about which facts, quotes, or sections fail to meet audit-level verification standards.')\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return True if self.status == \"approved\" else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agents-config.yaml file\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "with open('./tmp/agents-config.yaml', 'r') as file:\n",
    "    agent_config_data = yaml.safe_load(file)\n",
    "\n",
    "formatted_json = json.dumps(agent_config_data, indent=4)\n",
    "print(formatted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd116726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from agents import Agent, ModelSettings, SQLiteSession\n",
    "\n",
    "def create_agent(agent_type: str = None):\n",
    "    \"\"\" \n",
    "    Creates and returns an Agent that matches the given definition in the agents-config \n",
    "    YAML file. Optionally returns a memory Session if agent configuration calls for it.\n",
    "    \"\"\"\n",
    "\n",
    "    if agent_type is None or not agent_type.strip():\n",
    "        raise ValueError(\"agent_type must be a valid type of agent defined in agent-configs.yaml.\")\n",
    "    \n",
    "    agent_config = agent_config_data.get(agent_type)\n",
    "    if agent_config is None:\n",
    "        raise ValueError(f\"'{agent_type}' does not match an agent defined in agent-configs.yaml.\")\n",
    "\n",
    "    # Generate a timestamp string for unique naming\n",
    "    now_string = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SU%s\")\n",
    "\n",
    "    # Build agent based on YAML specification\n",
    "    try:\n",
    "        agent_model_settings=ModelSettings(\n",
    "            temperature=agent_config.get('temperature'),\n",
    "            max_tokens=agent_config['max_tokens'],\n",
    "        )\n",
    "\n",
    "        agent_name = agent_config.get('name') or f\"{agent_type}_{now_string}\"\n",
    "        \n",
    "        new_agent = Agent(\n",
    "            name=agent_name,\n",
    "            instructions=agent_config['instructions'],\n",
    "            model=agent_config.get('model') or agent_model_DEFAULT,\n",
    "            output_type=globals().get(agent_config.get('output_type') or None),\n",
    "            model_settings=agent_model_settings\n",
    "        )\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    # Create memory session for agent if configured\n",
    "    agent_has_memory = agent_config.get('has_memory') or False\n",
    "    agent_session_name = f\"{agent_name}__SESSION_{now_string}\" if agent_has_memory else None\n",
    "    agent_session = SQLiteSession(agent_session_name) if agent_session_name else None\n",
    "\n",
    "    return (new_agent, agent_session)\n",
    "\n",
    "worker, worker_sess = create_agent('worker')\n",
    "validator, validator_sess = create_agent('validator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialog loop for a Worker agent and a Validoator agent. \n",
    "# This assigns a task to the Worker-Validator pair. The Worker attempts to complete the task to the best of its\n",
    "# ability. The Validator determines whether the Worker's output meets the success criteria for the \n",
    "# assigned task. If so, it returns \"VALIDATED\" and the output is returned. If not, it gives feedback to the\n",
    "# Worker agent, which must attempt the task again using the feedback.\n",
    "\n",
    "from agents import Runner, trace\n",
    "\n",
    "async def assign_task(task: str, success_criteria: str = None, max_loops: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Asssign a task to the Worker-Validator pair and receive their response in return.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that Worker agent exists\n",
    "    if worker is None:\n",
    "        return \"Worker agent has not been created.\"\n",
    "    if validator is None:\n",
    "        return \"Validator agent has not been created.\"\n",
    "\n",
    "    assignment = f\"\\ntask: {task}\\nsuccess_criteria: {success_criteria}\"\n",
    "    history = [{\"role\": \"user\", \"content\": assignment}]\n",
    "\n",
    "    count_loop = 0\n",
    "    output_validated = False\n",
    "    result = None\n",
    "\n",
    "    with trace(f\"{worker.name}_{validator.name}\"):\n",
    "        while (not output_validated) and (count_loop < max_loops):\n",
    "            count_loop+=1\n",
    "\n",
    "            try:\n",
    "                worker_output = await Runner.run(starting_agent=worker, input=history)\n",
    "                history.append({\"role\": \"assistant\", \"content\": worker_output.final_output})\n",
    "\n",
    "                validator_output = await Runner.run(starting_agent=validator, input=history)\n",
    "                assessment = validator_output.final_output\n",
    "                history.append({\"role\": \"assistant\", \"content\": assessment.model_dump_json()})\n",
    "\n",
    "            except Exception as e:\n",
    "                return f\"Error: {e}\"\n",
    "        \n",
    "            output_validated = assessment.is_valid()\n",
    "            result = worker_output.final_output if output_validated else assessment.model_dump_json()\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that implements the Worker-Validator pattern\n",
    "\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from agents import Agent, ModelSettings\n",
    "from agents import Runner, trace\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "class ValidatorResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the structured response from a validator agent.\n",
    "\n",
    "    This Pydantic model ensures the validator's output is consistent and machine-readable.\n",
    "    It includes a status indicating approval or a need for revision, a brief rationale,\n",
    "    and detailed feedback if the output requires changes.\n",
    "    \"\"\"\n",
    "\n",
    "    status: Literal[\"approved\", \"needs_revision\"]  = Field(..., description='Must be either \"approved\" or \"needs_revision\"')\n",
    "    rationale: str = Field(..., description='Summarise your evaluation in 10 words, referencing specific success criteria.')\n",
    "    feedback: Optional[str] = Field(None, description='Only include if status is \"needs_revision\". Provide concrete, actionable instructions for the Worker to correct or improve the output. Be explicit about which facts, quotes, or sections fail to meet audit-level verification standards.')\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return True if self.status == \"approved\" else False\n",
    "\n",
    "class WorkerValidatorPattern:\n",
    "    \"\"\"\n",
    "    A class that implements the Worker-Validator pattern for AI agent collaboration.\n",
    "\n",
    "    This pattern orchestrates a loop where a 'worker' agent performs a task and a 'validator'\n",
    "    agent evaluates the worker's output against a set of success criteria. The validator provides\n",
    "    feedback to the worker if the output needs revision. The process repeats until the\n",
    "    validator approves the output or a maximum number of loops is reached.\n",
    "\n",
    "    The class uses a YAML configuration file to define and initialize the worker and validator\n",
    "    agents. It maintains a history of the conversation, allowing agents to have context\n",
    "    from previous turns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent_config_filename: str = './tmp/agents-config.yaml') -> None:\n",
    "        \"\"\"\n",
    "        Initialize the pattern from the agents-config.yaml file\n",
    "        \"\"\"\n",
    "\n",
    "        self.__agent_model_DEFAULT = os.getenv('CONF_OPENAI_DEFAULT_MODEL')\n",
    "\n",
    "        # Load the agents-config.yaml file\n",
    "        with open(agent_config_filename, 'r') as file:\n",
    "            self.__agent_config_data = yaml.safe_load(file)\n",
    "        \n",
    "        # Create an empty history log\n",
    "        self.__max_history = 5\n",
    "        self.__history_log = deque(maxlen=self.__max_history)\n",
    "\n",
    "        # Create the agents for this pattern\n",
    "        self.__init_agents() \n",
    "\n",
    "\n",
    "    def __new_history(self) -> dict:\n",
    "        \"\"\"\n",
    "        Add a new empty conversation history to the log and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        history = []\n",
    "        self.__history_log.append(history)\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def get_history(self, age: int = 0) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Retrieves a stored history dictionary, using a zero-based index for age (0 is most recent).\n",
    "        \"\"\"\n",
    "        if not self.__history_log:\n",
    "            return None\n",
    "        \n",
    "        index = -1 * min((age + 1), len(self.__history_log))\n",
    "        try:\n",
    "            return self.__history_log[index]\n",
    "        except IndexError:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def __create_agent(self, agent_type: str = None) -> Agent:\n",
    "        \"\"\"\n",
    "        Creates and returns an Agent that matches the given definition in the agents-config YAML file.\n",
    "        \"\"\"\n",
    "\n",
    "        if agent_type is None or not agent_type.strip():\n",
    "            raise ValueError(\"agent_type must be a valid type of agent defined in agent-configs.yaml.\")\n",
    "        \n",
    "        agent_config = self.__agent_config_data.get(agent_type)\n",
    "        if agent_config is None:\n",
    "            raise ValueError(f\"'{agent_type}' does not match an agent defined in agent-configs.yaml.\")\n",
    "\n",
    "        # Generate a timestamp string for unique naming\n",
    "        now_string = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SU%s\")\n",
    "\n",
    "        # Build agent based on YAML specification\n",
    "        agent_model_settings=ModelSettings(\n",
    "            temperature=agent_config.get('temperature'),\n",
    "            max_tokens=agent_config.get('max_tokens')\n",
    "        )\n",
    "        \n",
    "        new_agent = Agent(\n",
    "            name=agent_config.get('name') or f\"{agent_type}_{now_string}\",\n",
    "            instructions=agent_config.get('instructions'),\n",
    "            model=agent_config.get('model') or self.__agent_model_DEFAULT,\n",
    "            output_type=globals().get(agent_config.get('output_type')),\n",
    "            model_settings=agent_model_settings\n",
    "        )\n",
    "\n",
    "        return new_agent\n",
    "    \n",
    "    \n",
    "    def __init_agents(self) -> int:\n",
    "        \"\"\"\n",
    "        Creates an instance of each of the agent types loaded from agents-config.yaml.\n",
    "        Returns the number of agents created.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.__agents = {}\n",
    "\n",
    "        for agent_type in self.__agent_config_data:\n",
    "            new_agent = self.__create_agent(agent_type=agent_type)\n",
    "        \n",
    "            if new_agent is not None:\n",
    "                self.__agents[agent_type] = new_agent\n",
    "\n",
    "        return len(self.__agents)\n",
    "    \n",
    "\n",
    "    async def assign_task(self, \n",
    "                          task: str, \n",
    "                          success_criteria: str = None, \n",
    "                          max_loops: int = 5) -> tuple[str, bool]:\n",
    "        \"\"\"\n",
    "        Assigns a task to a Worker-Validator pair and awaits a final response.\n",
    "\n",
    "        Args:\n",
    "            task (str): The primary task for the agents to execute.\n",
    "            success_criteria (str, optional): Describes the criteria to be used in validating the output.\n",
    "                                              Defaults to None.\n",
    "            max_loops (int, optional): The maximum number of validation loops to attempt.\n",
    "                                       Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            tuple[str, bool]: A tuple containing:\n",
    "                - str: The final, validated output from the Worker agent, or validator's failure \n",
    "                       assessment if output was not validated after maximum turns.\n",
    "                - bool: A flag indicating if the output was successfully validated.\n",
    "        \"\"\"\n",
    "\n",
    "        # Obtain worker and validator agents\n",
    "        worker = self.__agents.get(\"worker\")\n",
    "        validator = self.__agents.get(\"validator\")\n",
    " \n",
    "        if worker is None or validator is None:\n",
    "            raise NameError(\"One or more agents were not created. Are they correctly defined in agent-configs.yaml?\")\n",
    "\n",
    "        # Seed a new history dictionary\n",
    "        history = self.__new_history()\n",
    "        assignment = f\"task: {task}\"\n",
    "        if success_criteria is not None and len(success_criteria) > 0:\n",
    "            assignment += f\"\\nsuccess_criteria: {success_criteria}\"\n",
    "        history.append({\"role\": \"user\", \"content\": assignment})\n",
    "\n",
    "        # Loop between worker completing the task and validator checking it. Exit when validator approves\n",
    "        # or the number of iterations reaches max_loops.\n",
    "        count_loop = 0\n",
    "        output_validated = False\n",
    "        result = None\n",
    "\n",
    "        with trace(f\"{worker.name}_{validator.name}\"):\n",
    "            while (not output_validated) and (count_loop < max_loops):\n",
    "                count_loop+=1\n",
    "\n",
    "                try:\n",
    "                    worker_output = await Runner.run(starting_agent=worker, input=history)\n",
    "                    history.append({\"role\": \"assistant\", \"content\": worker_output.final_output})\n",
    "                except Exception as e:\n",
    "                    history.append({\"role\": \"user\", \"content\": f\"Error: {e}\"})\n",
    "\n",
    "                try:\n",
    "                    validator_output = await Runner.run(starting_agent=validator, input=history)\n",
    "                    assessment = validator_output.final_output_as(ValidatorResponse, raise_if_incorrect_type=True)\n",
    "                    history.append({\"role\": \"assistant\", \"content\": assessment.model_dump_json()})\n",
    "\n",
    "                    output_validated = assessment.is_valid()\n",
    "                    result = worker_output.final_output if output_validated else assessment.model_dump_json()\n",
    "                except Exception as e:\n",
    "                    history.append({\"role\": \"user\", \"content\": f\"Error: {e}\"})\n",
    "                    output_validated = False\n",
    "                    result = f\"Error: {e}\"\n",
    "            \n",
    "        return (result, output_validated)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradio interface to interact with the Worker-Validator pattern\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "use_pattern = False\n",
    "\n",
    "with gr.Blocks(title=\"Worker-Validator â€” Agentic Design Patterns\") as demo:\n",
    "\n",
    "    pattern = WorkerValidatorPattern(agent_config_filename='./tmp/agents-config.yaml') if use_pattern else None\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    # ðŸ› ï¸ Worker-Validator Pattern\n",
    "    \n",
    "    This is a simple worker-validator that can be used to perform single-step execution tasks whose output is validated before being returned.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=9):\n",
    "            task = gr.Textbox(label=\"Task\", placeholder=\"What task do you want completed?\", lines=4)\n",
    "            success_criteria = gr.Textbox(label=\"Success Criteria\", placeholder=\"Describe what success looks like.\", lines=4)\n",
    "        with gr.Column(scale=1):\n",
    "            max_loops = gr.Number(label=\"Maximum loops\", value=3)\n",
    "            run_btn = gr.Button(\"Run Task\", variant=\"primary\")\n",
    "    out = gr.Markdown(label=\"Result\", show_label=True)\n",
    "    history = gr.JSON(label=\"History\", show_label=True)\n",
    "\n",
    "    async def on_run(task:str, success_criteria: str, max_loops: int):\n",
    "        run_btn.interactive = False  # disable button while running\n",
    "        try:\n",
    "            output = None\n",
    "            hist = None\n",
    "            if use_pattern:\n",
    "                output, _ = await pattern.assign_task(task=task, success_criteria=success_criteria, max_loops=max_loops)\n",
    "                hist_dict = pattern.get_history()\n",
    "                hist = json.dumps(hist_dict, indent=2)\n",
    "            else:\n",
    "                output = await assign_task(task, success_criteria, max_loops)\n",
    "                hist = \"{}\"\n",
    "            return (output, hist)\n",
    "        finally:\n",
    "            run_btn.interactive = True  # re-enable button\n",
    "\n",
    "    run_btn.click(on_run, inputs=[task, success_criteria, max_loops], outputs=[out, history])\n",
    "    task.submit(on_run, inputs=[task, success_criteria, max_loops], outputs=[out, history])\n",
    "\n",
    "demo.launch(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\" \n",
    "Write a three-sentence executive summary of the financial performance of Tesla's Q1 2024 earnings. Your summary must focus on revenue, and it must explicitly state whether the Q1 revenue was higher or lower than the Q4 2023 revenue.\n",
    "\"\"\"\n",
    "\n",
    "criteria = \"\"\" \n",
    "The output must be a summary of Tesla's Q1 2024 earnings.\n",
    "The summary must be exactly three sentences long.\n",
    "The summary must explicitly mention the exact Q1 2024 revenue figure in USD.\n",
    "The summary must contain a clear statement comparing the Q1 2024 revenue to the Q4 2023 revenue (i.e., whether it was higher or lower).\n",
    "The summary must not contain the word \"profit\" or \"loss\".\n",
    "All factual data, including the revenue numbers and the comparison, must be accurate and verifiable from official sources.\n",
    "\"\"\"\n",
    "\n",
    "await assign_task(task, criteria)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
