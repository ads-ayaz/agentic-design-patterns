{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844816fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "agent_model_DEFAULT = os.getenv('CONF_OPENAI_DEFAULT_MODEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60306404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tmp/agents-config.yaml\n",
    "# agents-config.yaml\n",
    "worker:\n",
    "  name: Basic Worker\n",
    "  instructions: |\n",
    "    You are a single-step Worker. Execute the given task directly and return the best possible result.\n",
    "    Do not ask clarifying questions. Do not handoff, delegate, or call tools unless explicitly provided.\n",
    "    Prefer concise, correct outputs. Use Markdown formatting when helpful.\n",
    "  model: openai/gpt-4.1-nano\n",
    "  has_memory: False\n",
    "  temperature: 0.4\n",
    "  max_tokens: 1000\n",
    "\n",
    "validator:\n",
    "  name: Basic Validator\n",
    "  instructions: |\n",
    "    You are a Validator agent in a Worker–Validator loop. Your role is to **assess whether \n",
    "    the Worker’s output meets the task objective and success criteria**.\n",
    "    \n",
    "    Your job is **not to redo the research yourself**, but to:\n",
    "    1. Decide if the output is acceptable given the task and whatever success criteria is provided.\n",
    "    2. If not, give precise, actionable feedback so the Worker can correct it.\n",
    "  model: openai/gpt-4o-mini\n",
    "  temperature: 0.2\n",
    "  max_tokens: 1000\n",
    "  output_type: ValidatorResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317333a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structured output type for Validator\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "class ValidatorResponse(BaseModel):\n",
    "    status: Literal[\"approved\", \"needs_revision\"]  = Field(..., description='Must be either \"approved\" or \"needs_revision\"')\n",
    "    rationale: str = Field(..., description='Summarise your evaluation in 10 words, referencing specific success criteria.')\n",
    "    feedback: Optional[str] = Field(None, description='Only include if status is \"needs_revision\". Provide concrete, actionable instructions for the Worker to correct or improve the output. Be explicit about which facts, quotes, or sections fail to meet audit-level verification standards.')\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return True if self.status == \"approved\" else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agents-config.yaml file\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "with open('./tmp/agents-config.yaml', 'r') as file:\n",
    "    agent_config_data = yaml.safe_load(file)\n",
    "\n",
    "formatted_json = json.dumps(agent_config_data, indent=4)\n",
    "print(formatted_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd116726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from agents import Agent, ModelSettings, SQLiteSession\n",
    "\n",
    "def create_agent(agent_type: str = None):\n",
    "    \"\"\" \n",
    "    Creates and returns an Agent that matches the given definition in the agents-config \n",
    "    YAML file. Optionally returns a memory Session if agent configuration calls for it.\n",
    "    \"\"\"\n",
    "\n",
    "    if agent_type is None or not agent_type.strip():\n",
    "        raise ValueError(\"agent_type must be a valid type of agent defined in agent-configs.yaml.\")\n",
    "    \n",
    "    agent_config = agent_config_data.get(agent_type)\n",
    "    if agent_config is None:\n",
    "        raise ValueError(f\"'{agent_type}' does not match an agent defined in agent-configs.yaml.\")\n",
    "\n",
    "    # Generate a timestamp string for unique naming\n",
    "    now_string = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SU%s\")\n",
    "\n",
    "    # Build agent based on YAML specification\n",
    "    try:\n",
    "        agent_model_settings=ModelSettings(\n",
    "            temperature=agent_config.get('temperature'),\n",
    "            max_tokens=agent_config['max_tokens'],\n",
    "        )\n",
    "\n",
    "        agent_name = agent_config.get('name') or f\"{agent_type}_{now_string}\"\n",
    "        \n",
    "        new_agent = Agent(\n",
    "            name=agent_name,\n",
    "            instructions=agent_config['instructions'],\n",
    "            model=agent_config.get('model') or agent_model_DEFAULT,\n",
    "            output_type=globals().get(agent_config.get('output_type') or None),\n",
    "            model_settings=agent_model_settings\n",
    "        )\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    # Create memory session for agent if configured\n",
    "    agent_has_memory = agent_config.get('has_memory') or False\n",
    "    agent_session_name = f\"{agent_name}__SESSION_{now_string}\" if agent_has_memory else None\n",
    "    agent_session = SQLiteSession(agent_session_name) if agent_session_name else None\n",
    "\n",
    "    return (new_agent, agent_session)\n",
    "\n",
    "worker, worker_sess = create_agent('worker')\n",
    "validator, validator_sess = create_agent('validator')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialog loop for a Worker agent and a Validoator agent. \n",
    "# This assigns a task to the Worker-Validator pair. The Worker attempts to complete the task to the best of its\n",
    "# ability. The Validator determines whether the Worker's output meets the success criteria for the \n",
    "# assigned task. If so, it returns \"VALIDATED\" and the output is returned. If not, it gives feedback to the\n",
    "# Worker agent, which must attempt the task again using the feedback.\n",
    "\n",
    "from agents import Runner, trace\n",
    "\n",
    "async def assign_task(task: str, success_criteria: str = None, max_loops: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Asssign a task to the Worker-Validator pair and receive their response in return.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that Worker agent exists\n",
    "    if worker is None:\n",
    "        return \"Worker agent has not been created.\"\n",
    "    if validator is None:\n",
    "        return \"Validator agent has not been created.\"\n",
    "\n",
    "    assignment = f\"\\ntask: {task}\\nsuccess_criteria: {success_criteria}\"\n",
    "    history = [{\"role\": \"user\", \"content\": assignment}]\n",
    "\n",
    "    count_loop = 0\n",
    "    output_validated = False\n",
    "    result = None\n",
    "\n",
    "    with trace(f\"{worker.name}_{validator.name}\"):\n",
    "        while (not output_validated) and (count_loop < max_loops):\n",
    "            count_loop+=1\n",
    "\n",
    "            try:\n",
    "                worker_output = await Runner.run(starting_agent=worker, input=history)\n",
    "                history.append({\"role\": \"assistant\", \"content\": worker_output.final_output})\n",
    "\n",
    "                validator_output = await Runner.run(starting_agent=validator, input=history)\n",
    "                assessment = validator_output.final_output\n",
    "                history.append({\"role\": \"assistant\", \"content\": assessment.model_dump_json()})\n",
    "\n",
    "            except Exception as e:\n",
    "                return f\"Error: {e}\"\n",
    "        \n",
    "            output_validated = assessment.is_valid()\n",
    "            result = worker_output.final_output if output_validated else assessment.model_dump_json()\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradio interface to interact with the Worker-Validator pattern\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(title=\"Worker-Validator — Agentic Design Patterns\") as demo:\n",
    "\n",
    "    # wp = WorkerPattern(agent_has_memory=True) if use_worker_pattern else None\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🛠️ Worker-Validator Pattern\n",
    "    \n",
    "    This is a simple worker-validator that can be used to perform single-step execution tasks whose output is validated before being returned.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=9):\n",
    "            task = gr.Textbox(label=\"Task\", placeholder=\"What task do you want completed?\", lines=4)\n",
    "            success_criteria = gr.Textbox(label=\"Success Criteria\", placeholder=\"Describe what success looks like.\", lines=4)\n",
    "        with gr.Column(scale=1):\n",
    "            max_loops = gr.Number(label=\"Maximum loops\", value=3)\n",
    "            run_btn = gr.Button(\"Ask\", variant=\"primary\")\n",
    "    out = gr.Markdown(label=\"Result\")\n",
    "\n",
    "    async def on_run(task:str, success_criteria: str, max_loops: int):\n",
    "        run_btn.interactive = False  # disable button while running\n",
    "        try:\n",
    "            return await assign_task(task, success_criteria, max_loops)\n",
    "        finally:\n",
    "            run_btn.interactive = True  # re-enable button\n",
    "\n",
    "    run_btn.click(on_run, inputs=[task, success_criteria, max_loops], outputs=[out])\n",
    "    task.submit(on_run, inputs=[task, success_criteria, max_loops], outputs=[out])\n",
    "\n",
    "demo.launch(inline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\" \n",
    "Write a three-sentence executive summary of the financial performance of Tesla's Q1 2024 earnings. Your summary must focus on revenue, and it must explicitly state whether the Q1 revenue was higher or lower than the Q4 2023 revenue.\n",
    "\"\"\"\n",
    "\n",
    "criteria = \"\"\" \n",
    "The output must be a summary of Tesla's Q1 2024 earnings.\n",
    "The summary must be exactly three sentences long.\n",
    "The summary must explicitly mention the exact Q1 2024 revenue figure in USD.\n",
    "The summary must contain a clear statement comparing the Q1 2024 revenue to the Q4 2023 revenue (i.e., whether it was higher or lower).\n",
    "The summary must not contain the word \"profit\" or \"loss\".\n",
    "All factual data, including the revenue numbers and the comparison, must be accurate and verifiable from official sources.\n",
    "\"\"\"\n",
    "\n",
    "await assign_task(task, criteria)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
