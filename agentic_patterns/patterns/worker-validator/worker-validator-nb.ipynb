{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3a24922",
   "metadata": {},
   "source": [
    "# WORKER–VALIDATOR | Agentic Pattern\n",
    "\n",
    "A two-agent pattern that introduces a quality control loop. A Worker agent attempts the task, and a Validator agent reviews the result against the original instructions. If the result is inadequate, the Validator provides feedback, and the Worker revises the output. This loop continues until the Validator approves the result or a termination condition is reached.\n",
    "\n",
    "## Problem Addressed\n",
    "This pattern improves the reliability and quality of output for tasks that may require iterative refinement, quality assurance, or structured reasoning. It is especially useful when the cost of error is high or task success criteria are non-trivial to evaluate.\n",
    "\n",
    "## Pattern Structure\n",
    "\n",
    "**Agents |**\n",
    "- `Worker`: Interprets the task and produces an initial output.\n",
    "- `Validator`: Evaluates whether the Worker’s output satisfies the task requirements, and provides structured feedback if not.\n",
    "\n",
    "**Coordination Topology |**\n",
    "- Sequential feedback loop.\n",
    "- `Caller → Worker → Validator → (loop: Worker ← Validator)` until the Validator approves the result or a max iteration limit is reached.\n",
    "\n",
    "## Assumptions\n",
    "\n",
    "Worker:\n",
    "- Has all the assumptions of the Worker pattern (tools, reasoning, etc.).\n",
    "- Can **interpret and apply feedback** to revise prior outputs.\n",
    "\n",
    "Validator:\n",
    "- Possesses the capability to **interpret task requirements**, **evaluate the Worker’s output**, and **generate feedback or approval**.\n",
    "- Can compare against a reference prompt or desired criteria.\n",
    "\n",
    "Agents may or may not share memory. Coordination state must persist across iterations.\n",
    "\n",
    "## Inputs\n",
    "- A task description (e.g., instructions or goals).\n",
    "- Optionally: a rubric or evaluation criteria for the Validator.\n",
    "\n",
    "## Outputs\n",
    "- A final validated result.\n",
    "- Optionally: Validator’s rationale or evaluation score.\n",
    "\n",
    "## Behavioural Flow\n",
    "\n",
    "1. Input task is passed to the Worker.\n",
    "2. Worker generates initial output.\n",
    "3. Output and task are passed to the Validator.\n",
    "4. Validator evaluates the output:\n",
    "    - If acceptable, the result is returned.\n",
    "    - If not, Validator returns structured feedback.\n",
    "5. Worker revises output based on feedback.\n",
    "6. Loop returns to step 3. Repeat until approval or max iterations reached.\n",
    "\n",
    "## Strengths\n",
    "- Higher output quality and reliability.\n",
    "- Supports more complex, ambiguous, or high-stakes tasks.\n",
    "- Validator role introduces an externalized standard of correctness or fit.\n",
    "\n",
    "## Weaknesses\n",
    "- Higher latency and compute cost.\n",
    "- Requires well-aligned Validator to avoid false negatives or noise.\n",
    "- Coordination complexity increases with number of iterations.\n",
    "\n",
    "## Variations and Extensions\n",
    "- **Multi-round Validator**: Uses temperature annealing or scoring thresholds to control the feedback loop.\n",
    "- **Rotating Roles**: Worker and Validator swap roles periodically to simulate peer review.\n",
    "- **Planner-Worker-Validator**: Validator ensures both planning and execution meet the goal.\n",
    "\n",
    "## Example Use Cases\n",
    "- Generate a product description from specifications.\n",
    "- Answer a factual question using embedded knowledge.\n",
    "- Write a function to perform a specific operation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da2301",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474e91e",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "\n",
    "This code prepares the notebook to run by loading configuration variables from a `.env` file in the project root.  \n",
    "- The `.env` file contains private credentials such as the `OPENAI_API_KEY` required to authenticate with the OpenAI API.  \n",
    "- It also specifies the default model to use (e.g., `CONF_OPENAI_DEFAULT_MODEL=openai/gpt-4o-mini`).  \n",
    "\n",
    "We use `python-dotenv` to automatically load these values into the environment at runtime. This keeps secrets out of the codebase and makes it easy to switch models or keys without modifying the notebook.  \n",
    "\n",
    "**Key points**  \n",
    "- Make sure `.env` exists before running the notebook.  \n",
    "- At minimum, define `OPENAI_API_KEY=sk-proj-XXXX`.  \n",
    "- Optionally, set `CONF_OPENAI_DEFAULT_MODEL` to specify which model Worker and Validator agents will default to.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844816fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "agent_model_DEFAULT = os.getenv('CONF_OPENAI_DEFAULT_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e7b625",
   "metadata": {},
   "source": [
    "### Agent configuration file\n",
    "\n",
    "The next two cells first create a temporary working directory and then write an `agents-config.yaml` file that specifies the Worker and Validator agents used in the pattern. By defining agents declaratively in YAML rather than embedding them directly in Python, the configuration becomes easier to read, test, and adjust. This separation also makes experiments reproducible, since the roles and their instructions can be version-controlled and swapped without changing the implementation code.\n",
    "\n",
    "In the Worker–Validator pattern, this configuration provides a clear contract for how each agent should behave. The Worker is tuned to focus on execution, producing a direct output without unnecessary deliberation. The Validator is constrained to assessing whether the Worker’s response meets the task requirements and providing targeted feedback. Distinct settings for each role make the loop both flexible and controlled. For instance, the Worker can be given a slightly higher temperature to allow for more exploration, while the Validator runs with a lower temperature for stable, consistent evaluation.\n",
    "\n",
    "**Key fields in YAML**  \n",
    "- `name` – human-readable label for logs and traces  \n",
    "- `instructions` – role prompt, with the Worker constrained to execution and the Validator to judgement  \n",
    "- `model` – the model id for each role (they may differ)  \n",
    "- `has_memory` – whether the agent keeps state across turns (set to False for the Worker here)  \n",
    "- `temperature` – balance of exploration vs determinism  \n",
    "- `max_tokens` – cap on response length  \n",
    "- `output_type` – expected schema or type of output (e.g., `ValidatorResponse`)  \n",
    "\n",
    "The design choice to externalize configuration improves portability and auditability but introduces a dependency on external files and the need for careful formatting. YAML’s indentation rules in particular can cause issues if not handled precisely.  \n",
    "\n",
    "**Gotchas and extensions**  \n",
    "- Ensure the model ids exist in your runtime/provider and that your API key has access  \n",
    "- Make sure that the class corresponding to `output_type` is defined in the code and visible at the global level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e138f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60306404",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tmp/agents-config.yaml\n",
    "# agents-config.yaml\n",
    "worker:\n",
    "  name: Basic Worker\n",
    "  instructions: |\n",
    "    You are a single-step Worker. Execute the given task directly and return the best possible result.\n",
    "    Do not ask clarifying questions. Do not handoff, delegate, or call tools unless explicitly provided.\n",
    "    Prefer concise, correct outputs. Use Markdown formatting when helpful.\n",
    "  model: openai/gpt-4.1-nano\n",
    "  has_memory: False\n",
    "  temperature: 0.4\n",
    "  max_tokens: 1000\n",
    "\n",
    "validator:\n",
    "  name: Basic Validator\n",
    "  instructions: |\n",
    "    You are a Validator agent in a Worker–Validator loop. Your role is to **assess whether \n",
    "    the Worker’s output meets the task objective and success criteria**.\n",
    "    \n",
    "    Your job is **not to redo the research yourself**, but to:\n",
    "    1. Decide if the output is acceptable given the task and whatever success criteria is provided.\n",
    "    2. If not, give precise, actionable feedback so the Worker can correct it.\n",
    "  model: openai/gpt-4o-mini\n",
    "  temperature: 0.2\n",
    "  max_tokens: 1000\n",
    "  output_type: ValidatorResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b73fa1f",
   "metadata": {},
   "source": [
    "### ValidatorResponse class\n",
    "\n",
    "This cell defines the `ValidatorResponse` class, a Pydantic model that enforces structure on the Validator’s output. Instead of returning free-form text, the Validator must respond in a consistent schema that can be parsed and acted upon. This ensures the feedback loop between Worker and Validator is machine-checkable, making it possible to automate approval, rejection, and revision cycles.\n",
    "\n",
    "The schema captures three elements:  \n",
    "- `status` signals whether the Worker’s output is approved or needs revision.  \n",
    "- `rationale` provides a concise justification tied to the task’s success criteria.  \n",
    "- `feedback` is optional and included only when revisions are needed, giving the Worker actionable instructions.  \n",
    "\n",
    "This design keeps Validator outputs predictable and reduces the chance of misinterpretation. The `is_valid` method provides a quick way to check whether the output has been approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317333a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a structured output type for Validator\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "class ValidatorResponse(BaseModel):\n",
    "    status: Literal[\"approved\", \"needs_revision\"]  = Field(..., description='Must be either \"approved\" or \"needs_revision\"')\n",
    "    rationale: str = Field(..., description='Summarise your evaluation in 10 words, referencing specific success criteria.')\n",
    "    feedback: Optional[str] = Field(None, description='Only include if status is \"needs_revision\". Provide concrete, actionable instructions for the Worker to correct or improve the output. Be explicit about which facts, quotes, or sections fail to meet audit-level verification standards.')\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return True if self.status == \"approved\" else False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d833f03d",
   "metadata": {},
   "source": [
    "### Load and verify agent configuration\n",
    "\n",
    "This cell reads the `agents-config.yaml` file created earlier and converts it into a Python dictionary. The configuration is then printed as formatted JSON so the notebook user can confirm it was parsed correctly and matches expectations. This step is primarily a sanity check: it ensures that both the Worker and Validator agents are defined as intended before they are instantiated. If the YAML has formatting issues or the fields are mis-specified, this is the point where they can be caught and corrected.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4bff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the agents-config.yaml file\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "with open('./tmp/agents-config.yaml', 'r') as file:\n",
    "    agent_config_data = yaml.safe_load(file)\n",
    "\n",
    "formatted_json = json.dumps(agent_config_data, indent=4)\n",
    "print(formatted_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6e94d",
   "metadata": {},
   "source": [
    "### Create agents from configuration\n",
    "\n",
    "This cell defines a helper function, `create_agent`, which builds an agent directly from the specifications in `agents-config.yaml`. The function generalizes agent creation so that any role defined in the configuration file—Worker, Validator, or future additions—can be instantiated in the same way.\n",
    "\n",
    "The function first validates that the requested `agent_type` exists in the YAML. It then constructs a unique agent name with a timestamp, applies the role’s instructions and model, and sets model parameters such as temperature and maximum tokens. If the configuration specifies an `output_type`, the function maps it to a class defined in the notebook (e.g., `ValidatorResponse`). If the agent is configured to use memory, it creates a `SQLiteSession` to persist state across iterations.\n",
    "\n",
    "A timestamp string (`now_string`) is used to generate unique identifiers for agent names and, where applicable, memory sessions. This prevents collisions when multiple agents of the same type are created, and makes log traces easier to differentiate. More generally, the function is designed to fall back to sensible defaults—such as using a timestamp-based name or the default model—when values are not explicitly provided in the YAML, balancing flexibility with reliability.\n",
    "\n",
    "At the end of the cell, the function is used to instantiate both the Worker and Validator agents. This step effectively activates the configuration defined earlier, giving us two ready-to-use agents aligned to their roles in the Worker–Validator loop.\n",
    "\n",
    "#### Note on Session-based memory in the Worker-Validator pattern\n",
    "\n",
    "Although the function supports creating session-based memory through `SQLiteSession`, neither the Worker nor the Validator in this pattern require it. The Worker operates in a stateless manner, producing outputs solely from the given task, while the Validator only needs to judge the latest result against the original instructions. \n",
    "\n",
    "Memory is included here as a configurable option because the same mechanism may be useful in other patterns—for example, when agents need to track progress across multiple subtasks or carry forward context. Our broader intent is to keep agent configuration decoupled from the code that creates and executes them, so this framework can be reused and extended as additional agentic patterns are developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd116726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from agents import Agent, ModelSettings, SQLiteSession\n",
    "\n",
    "def create_agent(agent_type: str = None):\n",
    "    \"\"\" \n",
    "    Creates and returns an Agent that matches the given definition in the agents-config \n",
    "    YAML file. Optionally returns a memory Session if agent configuration calls for it.\n",
    "    \"\"\"\n",
    "\n",
    "    if agent_type is None or not agent_type.strip():\n",
    "        raise ValueError(\"agent_type must be a valid type of agent defined in agent-configs.yaml.\")\n",
    "    \n",
    "    agent_config = agent_config_data.get(agent_type)\n",
    "    if agent_config is None:\n",
    "        raise ValueError(f\"'{agent_type}' does not match an agent defined in agent-configs.yaml.\")\n",
    "\n",
    "    # Generate a timestamp string for unique naming\n",
    "    now_string = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SU%s\")\n",
    "\n",
    "    # Build agent based on YAML specification\n",
    "    try:\n",
    "        agent_model_settings=ModelSettings(\n",
    "            temperature=agent_config.get('temperature'),\n",
    "            max_tokens=agent_config['max_tokens'],\n",
    "        )\n",
    "\n",
    "        agent_name = agent_config.get('name') or f\"{agent_type}_{now_string}\"\n",
    "        \n",
    "        new_agent = Agent(\n",
    "            name=agent_name,\n",
    "            instructions=agent_config['instructions'],\n",
    "            model=agent_config.get('model') or agent_model_DEFAULT,\n",
    "            output_type=globals().get(agent_config.get('output_type') or None),\n",
    "            model_settings=agent_model_settings\n",
    "        )\n",
    "    except:\n",
    "        raise\n",
    "\n",
    "    # Create memory session for agent if configured\n",
    "    agent_has_memory = agent_config.get('has_memory') or False\n",
    "    agent_session_name = f\"{agent_name}__SESSION_{now_string}\" if agent_has_memory else None\n",
    "    agent_session = SQLiteSession(agent_session_name) if agent_session_name else None\n",
    "\n",
    "    return (new_agent, agent_session)\n",
    "\n",
    "worker, worker_sess = create_agent('worker')\n",
    "validator, validator_sess = create_agent('validator')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dc5868",
   "metadata": {},
   "source": [
    "### Worker–Validator loop\n",
    "\n",
    "This cell implements the control loop that coordinates the Worker and Validator agents. The function `assign_task` accepts a task description, optional success criteria, and a maximum number of iterations. It then orchestrates the dialogue: the Worker attempts the task, the Validator evaluates the output, and if the Validator requests revisions then the Worker receives feedback and tries again. The loop continues until the Validator approves the output or the iteration limit is reached.\n",
    "\n",
    "The function constructs an interaction history that records the task, Worker outputs, and Validator assessments. This history is passed back into each subsequent round, giving both agents visibility into what has already been attempted. The `trace` context manager is used to record the interaction, which helps with debugging or later analysis.\n",
    "\n",
    "Within the loop, the Worker produces an initial response which is stored in history. The Validator then runs against the same history and returns a structured `ValidatorResponse`. If the Validator approves the result, the function returns the Worker’s final output. Otherwise, the Validator’s feedback is appended to history, and the Worker is asked to try again.\n",
    "\n",
    "A `max_loops` safeguard ensures that the process cannot run indefinitely if the Worker fails to satisfy the Validator. This provides a balance between quality control and computational cost, keeping the loop bounded even when success criteria are difficult to meet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4880d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialog loop for a Worker agent and a Validoator agent. \n",
    "\n",
    "from agents import Runner, trace\n",
    "\n",
    "async def assign_task(task: str, success_criteria: str = None, max_loops: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Asssign a task to the Worker-Validator pair and receive their response in return.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that Worker agent exists\n",
    "    if worker is None:\n",
    "        return \"Worker agent has not been created.\"\n",
    "    if validator is None:\n",
    "        return \"Validator agent has not been created.\"\n",
    "\n",
    "    assignment = f\"\\ntask: {task}\\nsuccess_criteria: {success_criteria}\"\n",
    "    history = [{\"role\": \"user\", \"content\": assignment}]\n",
    "\n",
    "    count_loop = 0\n",
    "    output_validated = False\n",
    "    result = None\n",
    "\n",
    "    with trace(f\"{worker.name}_{validator.name}\"):\n",
    "        while (not output_validated) and (count_loop < max_loops):\n",
    "            count_loop+=1\n",
    "\n",
    "            try:\n",
    "                worker_output = await Runner.run(starting_agent=worker, input=history)\n",
    "                history.append({\"role\": \"assistant\", \"content\": worker_output.final_output})\n",
    "\n",
    "                validator_output = await Runner.run(starting_agent=validator, input=history)\n",
    "                assessment = validator_output.final_output\n",
    "                history.append({\"role\": \"assistant\", \"content\": assessment.model_dump_json()})\n",
    "\n",
    "            except Exception as e:\n",
    "                return f\"Error: {e}\"\n",
    "        \n",
    "            output_validated = assessment.is_valid()\n",
    "            result = worker_output.final_output if output_validated else assessment.model_dump_json()\n",
    "\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999c6458",
   "metadata": {},
   "source": [
    "### Example execution of the cell-based loop\n",
    "\n",
    "This cell demonstrates the cell-based `assign_task` function in action, before the standalone class is introduced. A concrete task is given to the Worker–Validator pair: generate a three-sentence executive summary of Tesla’s Q1 2024 earnings, with strict requirements around length, revenue detail, and comparison to Q4 2023. The accompanying success criteria serve as the Validator’s rubric, defining exactly what counts as an acceptable response.\n",
    "\n",
    "By running this example, you can verify that the Worker produces an output, the Validator checks it against the criteria, and—if needed—the loop continues until the result is approved or the maximum iteration count is reached. This provides a working proof-of-concept that the Worker–Validator loop functions correctly in practice before moving on to the more modular class-based implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd25b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\" \n",
    "Write a three-sentence executive summary of the financial performance of Tesla's Q1 2024 earnings. Your summary must focus on revenue, and it must explicitly state whether the Q1 revenue was higher or lower than the Q4 2023 revenue.\n",
    "\"\"\"\n",
    "\n",
    "criteria = \"\"\" \n",
    "The output must be a summary of Tesla's Q1 2024 earnings.\n",
    "The summary must be exactly three sentences long.\n",
    "The summary must explicitly mention the exact Q1 2024 revenue figure in USD.\n",
    "The summary must contain a clear statement comparing the Q1 2024 revenue to the Q4 2023 revenue (i.e., whether it was higher or lower).\n",
    "The summary must not contain the word \"profit\" or \"loss\".\n",
    "All factual data, including the revenue numbers and the comparison, must be accurate and verifiable from official sources.\n",
    "\"\"\"\n",
    "\n",
    "await assign_task(task, criteria)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00840a98",
   "metadata": {},
   "source": [
    "### Standalone class implementation\n",
    "\n",
    "This cell re-implements the Worker–Validator pattern as a standalone class that could be moved into a `.py` source file. Instead of scattering responsibilities across multiple notebook cells, the class bundles agent creation, task assignment, and history management into a single cohesive interface. This makes the pattern reusable, testable, and easier to extend in future work.\n",
    "\n",
    "Several modifications are worth noting: \n",
    "- The class introduces a bounded `deque` to manage a rolling history of past interactions, allowing previous runs to be retrieved while preventing unbounded growth. \n",
    "- Agent creation is encapsulated within private methods, ensuring each role is consistently instantiated from the YAML configuration. \n",
    "- Unlike the earlier loop, the validator output here is parsed using `final_output_as` with a strict type check against `ValidatorResponse`. This enforces schema compliance and fails early if the Validator returns malformed output. \n",
    "- The `assign_task` method now returns a tuple containing both the result and a Boolean flag indicating whether validation succeeded, which provides clearer signalling to downstream code.\n",
    "\n",
    "Another change is that session-based memory is not implemented, even if the YAML specifies it. The Worker–Validator pattern does not require agents to retain state across tasks, since the Worker focuses only on the current assignment and the Validator evaluates that single attempt. By omitting memory here, the class avoids unnecessary complexity while leaving the configuration mechanism in place for patterns that may need it.\n",
    "\n",
    "Overall, this refactor preserves the same logic as the notebook cells but presents it in a form that is better suited for modular development. It illustrates the direction of treating these agentic patterns not as isolated experiments but as building blocks for a broader framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcc12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class that implements the Worker-Validator pattern\n",
    "\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from agents import Agent, ModelSettings\n",
    "from agents import Runner, trace\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "class ValidatorResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the structured response from a validator agent.\n",
    "\n",
    "    This Pydantic model ensures the validator's output is consistent and machine-readable.\n",
    "    It includes a status indicating approval or a need for revision, a brief rationale,\n",
    "    and detailed feedback if the output requires changes.\n",
    "    \"\"\"\n",
    "\n",
    "    status: Literal[\"approved\", \"needs_revision\"]  = Field(..., description='Must be either \"approved\" or \"needs_revision\"')\n",
    "    rationale: str = Field(..., description='Summarise your evaluation in 10 words, referencing specific success criteria.')\n",
    "    feedback: Optional[str] = Field(None, description='Only include if status is \"needs_revision\". Provide concrete, actionable instructions for the Worker to correct or improve the output. Be explicit about which facts, quotes, or sections fail to meet audit-level verification standards.')\n",
    "\n",
    "    def is_valid(self) -> bool:\n",
    "        return True if self.status == \"approved\" else False\n",
    "\n",
    "class WorkerValidatorPattern:\n",
    "    \"\"\"\n",
    "    A class that implements the Worker-Validator pattern for AI agent collaboration.\n",
    "\n",
    "    This pattern orchestrates a loop where a 'worker' agent performs a task and a 'validator'\n",
    "    agent evaluates the worker's output against a set of success criteria. The validator provides\n",
    "    feedback to the worker if the output needs revision. The process repeats until the\n",
    "    validator approves the output or a maximum number of loops is reached.\n",
    "\n",
    "    The class uses a YAML configuration file to define and initialize the worker and validator\n",
    "    agents. It maintains a history of the conversation, allowing agents to have context\n",
    "    from previous turns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, agent_config_filename: str = './tmp/agents-config.yaml') -> None:\n",
    "        \"\"\"\n",
    "        Initialize the pattern from the agents-config.yaml file\n",
    "        \"\"\"\n",
    "\n",
    "        self.__agent_model_DEFAULT = os.getenv('CONF_OPENAI_DEFAULT_MODEL')\n",
    "\n",
    "        # Load the agents-config.yaml file\n",
    "        with open(agent_config_filename, 'r') as file:\n",
    "            self.__agent_config_data = yaml.safe_load(file)\n",
    "        \n",
    "        # Create an empty history log\n",
    "        self.__max_history = 5\n",
    "        self.__history_log = deque(maxlen=self.__max_history)\n",
    "\n",
    "        # Create the agents for this pattern\n",
    "        self.__init_agents() \n",
    "\n",
    "\n",
    "    def __new_history(self) -> dict:\n",
    "        \"\"\"\n",
    "        Add a new empty conversation history to the log and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        history = []\n",
    "        self.__history_log.append(history)\n",
    "\n",
    "        return history\n",
    "    \n",
    "    def get_history(self, age: int = 0) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Retrieves a stored history dictionary, using a zero-based index for age (0 is most recent).\n",
    "        \"\"\"\n",
    "        if not self.__history_log:\n",
    "            return None\n",
    "        \n",
    "        index = -1 * min((age + 1), len(self.__history_log))\n",
    "        try:\n",
    "            return self.__history_log[index]\n",
    "        except IndexError:\n",
    "            return None\n",
    "        \n",
    "    \n",
    "    def __create_agent(self, agent_type: str = None) -> Agent:\n",
    "        \"\"\"\n",
    "        Creates and returns an Agent that matches the given definition in the agents-config YAML file.\n",
    "        \"\"\"\n",
    "\n",
    "        if agent_type is None or not agent_type.strip():\n",
    "            raise ValueError(\"agent_type must be a valid type of agent defined in agent-configs.yaml.\")\n",
    "        \n",
    "        agent_config = self.__agent_config_data.get(agent_type)\n",
    "        if agent_config is None:\n",
    "            raise ValueError(f\"'{agent_type}' does not match an agent defined in agent-configs.yaml.\")\n",
    "\n",
    "        # Generate a timestamp string for unique naming\n",
    "        now_string = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SU%s\")\n",
    "\n",
    "        # Build agent based on YAML specification\n",
    "        agent_model_settings=ModelSettings(\n",
    "            temperature=agent_config.get('temperature'),\n",
    "            max_tokens=agent_config.get('max_tokens')\n",
    "        )\n",
    "        \n",
    "        new_agent = Agent(\n",
    "            name=agent_config.get('name') or f\"{agent_type}_{now_string}\",\n",
    "            instructions=agent_config.get('instructions'),\n",
    "            model=agent_config.get('model') or self.__agent_model_DEFAULT,\n",
    "            output_type=globals().get(agent_config.get('output_type')),\n",
    "            model_settings=agent_model_settings\n",
    "        )\n",
    "\n",
    "        return new_agent\n",
    "    \n",
    "    \n",
    "    def __init_agents(self) -> int:\n",
    "        \"\"\"\n",
    "        Creates an instance of each of the agent types loaded from agents-config.yaml.\n",
    "        Returns the number of agents created.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.__agents = {}\n",
    "\n",
    "        for agent_type in self.__agent_config_data:\n",
    "            new_agent = self.__create_agent(agent_type=agent_type)\n",
    "        \n",
    "            if new_agent is not None:\n",
    "                self.__agents[agent_type] = new_agent\n",
    "\n",
    "        return len(self.__agents)\n",
    "    \n",
    "\n",
    "    async def assign_task(self, \n",
    "                          task: str, \n",
    "                          success_criteria: str = None, \n",
    "                          max_loops: int = 5) -> tuple[str, bool]:\n",
    "        \"\"\"\n",
    "        Assigns a task to a Worker-Validator pair and awaits a final response.\n",
    "\n",
    "        Args:\n",
    "            task (str): The primary task for the agents to execute.\n",
    "            success_criteria (str, optional): Describes the criteria to be used in validating the output.\n",
    "                                              Defaults to None.\n",
    "            max_loops (int, optional): The maximum number of validation loops to attempt.\n",
    "                                       Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            tuple[str, bool]: A tuple containing:\n",
    "                - str: The final, validated output from the Worker agent, or validator's failure \n",
    "                       assessment if output was not validated after maximum turns.\n",
    "                - bool: A flag indicating if the output was successfully validated.\n",
    "        \"\"\"\n",
    "\n",
    "        # Obtain worker and validator agents\n",
    "        worker = self.__agents.get(\"worker\")\n",
    "        validator = self.__agents.get(\"validator\")\n",
    " \n",
    "        if worker is None or validator is None:\n",
    "            raise NameError(\"One or more agents were not created. Are they correctly defined in agent-configs.yaml?\")\n",
    "\n",
    "        # Seed a new history dictionary\n",
    "        history = self.__new_history()\n",
    "        assignment = f\"task: {task}\"\n",
    "        if success_criteria is not None and len(success_criteria) > 0:\n",
    "            assignment += f\"\\nsuccess_criteria: {success_criteria}\"\n",
    "        history.append({\"role\": \"user\", \"content\": assignment})\n",
    "\n",
    "        # Loop between worker completing the task and validator checking it. Exit when validator approves\n",
    "        # or the number of iterations reaches max_loops.\n",
    "        count_loop = 0\n",
    "        output_validated = False\n",
    "        result = None\n",
    "\n",
    "        with trace(f\"{worker.name}_{validator.name}\"):\n",
    "            while (not output_validated) and (count_loop < max_loops):\n",
    "                count_loop+=1\n",
    "\n",
    "                try:\n",
    "                    worker_output = await Runner.run(starting_agent=worker, input=history)\n",
    "                    history.append({\"role\": \"assistant\", \"content\": worker_output.final_output})\n",
    "                except Exception as e:\n",
    "                    history.append({\"role\": \"user\", \"content\": f\"Error: {e}\"})\n",
    "\n",
    "                try:\n",
    "                    validator_output = await Runner.run(starting_agent=validator, input=history)\n",
    "                    assessment = validator_output.final_output_as(ValidatorResponse, raise_if_incorrect_type=True)\n",
    "                    history.append({\"role\": \"assistant\", \"content\": assessment.model_dump_json()})\n",
    "\n",
    "                    output_validated = assessment.is_valid()\n",
    "                    result = worker_output.final_output if output_validated else assessment.model_dump_json()\n",
    "                except Exception as e:\n",
    "                    history.append({\"role\": \"user\", \"content\": f\"Error: {e}\"})\n",
    "                    output_validated = False\n",
    "                    result = f\"Error: {e}\"\n",
    "            \n",
    "        return (result, output_validated)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d5d22",
   "metadata": {},
   "source": [
    "### Gradio demo interface\n",
    "\n",
    "The final cell provides a simple Gradio interface for trying out the Worker–Validator pattern interactively. The UI lets you enter a task, optional success criteria, and a maximum number of validation loops. When you click **Run Task**, the Worker attempts the task, the Validator reviews it, and the result is displayed.\n",
    "\n",
    "The `use_pattern` flag determines which implementation is used:\n",
    "- If set to `True`, the demo runs with the class-based `WorkerValidatorPattern`. This version also displays the recorded interaction history, making it easier to see how the Worker and Validator collaborated.  \n",
    "- If set to `False`, the demo uses the earlier notebook functions. This mode is simpler but does not track history.  \n",
    "\n",
    "In addition to the Gradio interface, execution can be monitored through [OpenAI traces](https://platform.openai.com/traces). Each run within the loop is wrapped with a `trace` context, allowing you to inspect the flow of messages between Worker and Validator, debug errors, and analyse performance in the OpenAI dashboard. This provides visibility into how the pattern operates under the hood and helps refine prompts, parameters, and loop design.  \n",
    "\n",
    "By wrapping the pattern in a Gradio app, this cell turns the Worker–Validator design from an abstract loop into a tangible demo. It is especially useful for experimenting with different tasks, tweaking success criteria, and observing how many iterations the loop requires to reach an approved result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d38fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a gradio interface to interact with the Worker-Validator pattern\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "use_pattern = True\n",
    "\n",
    "with gr.Blocks(title=\"Worker-Validator — Agentic Design Patterns\") as demo:\n",
    "\n",
    "    pattern = WorkerValidatorPattern(agent_config_filename='./tmp/agents-config.yaml') if use_pattern else None\n",
    "\n",
    "    gr.Markdown(\"\"\"\n",
    "    # 🛠️ Worker-Validator Pattern\n",
    "    \n",
    "    This is a simple worker-validator that can be used to perform single-step execution tasks whose output is validated before being returned.\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=9):\n",
    "            task = gr.Textbox(label=\"Task\", placeholder=\"What task do you want completed?\", lines=4)\n",
    "            success_criteria = gr.Textbox(label=\"Success Criteria\", placeholder=\"Describe what success looks like.\", lines=4)\n",
    "        with gr.Column(scale=1):\n",
    "            max_loops = gr.Number(label=\"Maximum loops\", value=3)\n",
    "            run_btn = gr.Button(\"Run Task\", variant=\"primary\")\n",
    "    out = gr.Markdown(label=\"Result\", show_label=True)\n",
    "    history = gr.JSON(label=\"History\", show_label=True)\n",
    "\n",
    "    async def on_run(task:str, success_criteria: str, max_loops: int):\n",
    "        run_btn.interactive = False  # disable button while running\n",
    "        try:\n",
    "            output = None\n",
    "            hist = None\n",
    "            if use_pattern:\n",
    "                output, _ = await pattern.assign_task(task=task, success_criteria=success_criteria, max_loops=max_loops)\n",
    "                hist_dict = pattern.get_history()\n",
    "                hist = json.dumps(hist_dict, indent=2)\n",
    "            else:\n",
    "                output = await assign_task(task, success_criteria, max_loops)\n",
    "                hist = \"{}\"\n",
    "            return (output, hist)\n",
    "        finally:\n",
    "            run_btn.interactive = True  # re-enable button\n",
    "\n",
    "    run_btn.click(on_run, inputs=[task, success_criteria, max_loops], outputs=[out, history])\n",
    "    task.submit(on_run, inputs=[task, success_criteria, max_loops], outputs=[out, history])\n",
    "\n",
    "demo.launch(inline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
